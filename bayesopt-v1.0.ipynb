{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import test_functions\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%load_ext line_profiler\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch import optim, nn\n",
    "from gpytorch.kernels import RBFKernel, GridInterpolationKernel\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.random_variables import GaussianRandomVariable\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exact_RBF_GP(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, bounds):\n",
    "        super(Exact_RBF_GP, self).__init__(train_x, train_y, likelihood)\n",
    "        # Our mean function is constant in the interval [-1,1]\n",
    "        self.mean_module = ConstantMean(constant_bounds=(-1, 1))\n",
    "        # We use the RBF kernel as a universal approximator\n",
    "        self.covar_module = RBFKernel(log_lengthscale_bounds=(-5, 5))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        # Return moddl output as GaussianRandomVariable\n",
    "        return GaussianRandomVariable(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interp_RBF_GP(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, bounds):\n",
    "        super(Interp_RBF_GP, self).__init__(train_x, train_y, likelihood)\n",
    "        d = len(bounds)\n",
    "        # Near-zero mean\n",
    "        self.mean_module = ConstantMean(constant_bounds=[-1e-5,1e-5])\n",
    "        # GridInterpolationKernel over an ExactGP\n",
    "        self.base_covar_module = RBFKernel(log_lengthscale_bounds=(-5, 6))\n",
    "        self.covar_module = GridInterpolationKernel(self.base_covar_module, grid_size=60//d,\n",
    "                                                    grid_bounds=bounds)\n",
    "        # Register the log lengthscale as a trainable parametre\n",
    "        self.register_parameter('log_outputscale', nn.Parameter(torch.Tensor([0])), bounds=(-5,6))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        covar_x = covar_x.mul(self.log_outputscale.exp())\n",
    "        return GaussianRandomVariable(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EI_Acq(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, gp, y_max, xi, bounds):\n",
    "        super(EI_Acq, self).__init__()\n",
    "        dim = len(bounds)\n",
    "        \n",
    "        sample = torch.Tensor(1, 1).uniform_(bounds[0][0], bounds[0][1])\n",
    "        for i in range(1, len(bounds)):\n",
    "            sample = torch.cat([sample, torch.Tensor(1, 1).uniform_(bounds[i][0], bounds[i][1])], dim=1)\n",
    "        \n",
    "        self.x = nn.Parameter(sample)\n",
    "        self.gp = gp\n",
    "        self.xi = xi\n",
    "        self.y_max = y_max\n",
    "        \n",
    "    def forward(self):\n",
    "        with gpytorch.fast_pred_var():\n",
    "            predict_pt = self.gp(self.x)\n",
    "        mean = predict_pt.mean()\n",
    "        std = torch.sqrt(predict_pt.var())\n",
    "        z = (mean - self.y_max - self.xi)/std\n",
    "        return (\n",
    "            (mean - self.y_max - self.xi) * Variable(torch.from_numpy(norm.cdf(z.data)).float()) + \n",
    "            std * Variable(torch.from_numpy(norm.pdf(z.data)).float())\n",
    "            ) \n",
    "        \n",
    "        return self.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bayes_opt:\n",
    "    \n",
    "    def __init__(self, func, bounds, step_num):\n",
    "        \n",
    "        self.dim = len(bounds)\n",
    "        self.bounds = bounds\n",
    "        self.step_num = step_num    \n",
    "        self.obj_func = func\n",
    "        self.xi = 0.01\n",
    "        self.model_GP = None\n",
    "        \n",
    "        #Allocate space for iterates, initialize algorithm\n",
    "        start_num = 2\n",
    "        sample = torch.Tensor(start_num, self.dim)\n",
    "        for i in range(self.dim):\n",
    "            sample[:, i] = torch.Tensor(start_num, 1).uniform_(bounds[i][0], bounds[i][1])\n",
    "        \n",
    "        self.sample_pts = Variable(torch.zeros(self.step_num, self.dim))     \n",
    "        self.sample_vals = Variable(torch.zeros(self.step_num))\n",
    "        start_pts = Variable(sample)\n",
    "        start_vals = self.obj_func(start_pts)\n",
    "        self.sample_pts[:start_num] = start_pts\n",
    "        self.sample_vals[:start_num] = start_vals\n",
    "        \n",
    "        ind = torch.min(start_vals.data, 0)[1]\n",
    "        self.opt_val = torch.min(start_vals.data)\n",
    "        self.opt_soln = start_pts[ind]\n",
    "        self.model_GP = self._update_GP(start_pts, start_vals)\n",
    "        print(\"GP initiated\")\n",
    "        \n",
    "    \n",
    "    def _acq_max(self):\n",
    "        \n",
    "        tol = 10^-1\n",
    "        n_repeats = 10\n",
    "\n",
    "        for rep in range(n_repeats):\n",
    "\n",
    "            f = EI_Acq(self.model_GP, self.opt_val, self.xi, self.bounds)\n",
    "            optimizer = torch.optim.Adam([{'params': f.x}], lr=0.01/self.dim)\n",
    "            prev_loss = 0\n",
    "            bound_flag = True\n",
    "\n",
    "            for t in range(50):\n",
    "                optimizer.zero_grad()\n",
    "                loss = -f()\n",
    "                loss.backward(retain_graph=True)\n",
    "                optimizer.step()\n",
    "\n",
    "                if ((loss-prev_loss)**2 < tol).data.numpy(): break\n",
    "                prev_loss = loss\n",
    "\n",
    "            for i in range(self.dim):\n",
    "                if (f.x[:, i] < self.bounds[i][0]).any() or (f.x[:, i] > self.bounds[i][1]).any():\n",
    "                    bound_flag = False\n",
    "                    break\n",
    "                \n",
    "            if bound_flag == True:\n",
    "                break\n",
    "                \n",
    "                \n",
    "        clamped_pt = torch.Tensor(1, self.dim)\n",
    "        for i in range(self.dim):\n",
    "            clamped_pt[:, i] = torch.clamp(f.x.data[:, i], self.bounds[i][0], self.bounds[i][1])\n",
    "        \n",
    "        return Variable(clamped_pt)\n",
    "    \n",
    "    def _update_GP(self, train_x, train_y, train_hyperparams=False):\n",
    "        \n",
    "        likelihood = GaussianLikelihood()\n",
    "        model = Exact_RBF_GP(train_x.data, train_y.data, likelihood, self.bounds)\n",
    "        \n",
    "        if not self.model_GP is None:\n",
    "            model.parameters = self.model_GP.parameters\n",
    "\n",
    "        if train_hyperparams:\n",
    "            model.train()\n",
    "            likelihood.train()\n",
    "            # Use the adam optimizer\n",
    "            optimizer = torch.optim.Adam([\n",
    "                {'params': model.parameters()},  # Includes GaussianLikelihood parameters\n",
    "            ], lr=0.1)\n",
    "\n",
    "            # \"Loss\" for GPs - the marginal log likelihood\n",
    "            mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "            training_iter = 50\n",
    "            for i in range(training_iter):\n",
    "                # Zero gradients from previous iteration\n",
    "                optimizer.zero_grad()\n",
    "                # Output from model\n",
    "                output = model(train_x)\n",
    "                # Calc loss and backprop gradients\n",
    "                loss = -mll(output, train_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "    \n",
    "        # Put model and likelihood into eval mode\n",
    "        model.eval()\n",
    "        likelihood.eval()\n",
    "\n",
    "        return model\n",
    "        \n",
    "        \n",
    "    def optimize(self, mode=\"compact\"):\n",
    "        \n",
    "        start_num = int(self.sample_pts.nonzero().size(0)/self.dim)\n",
    "        if mode==\"eval\":\n",
    "            self.obj_hist = Variable(torch.zeros(step_num))\n",
    "            self.obj_hist[:start_num] = self.opt_val\n",
    "        \n",
    "        for t in range(start_num, self.step_num):\n",
    "            \n",
    "            #Get next point from acquisition function, evaluate objective\n",
    "            new_pt = self._acq_max()\n",
    "            new_val = float(self.obj_func(new_pt))\n",
    "            self.sample_pts[t] = new_pt\n",
    "            self.sample_vals[t] = new_val\n",
    "            \n",
    "            if new_val > self.opt_val:\n",
    "                self.opt_val = new_val\n",
    "                self.opt_soln = new_pt\n",
    "                \n",
    "            if mode==\"eval\":\n",
    "                self.obj_hist[t] = self.opt_val\n",
    "                \n",
    "            #Update GP with new observation\n",
    "            if t%25 == 0: \n",
    "                #print(t, ': ', self.opt_val)\n",
    "                self.model_GP = self._update_GP(self.sample_pts[:t+1], self.sample_vals[:t+1], train_hyperparams=True)\n",
    "            else:\n",
    "                self.model_GP = self._update_GP(self.sample_pts[:t+1], self.sample_vals[:t+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ackley_SGD(torch.nn.Module):\n",
    "    #Global max of 0 at origin\n",
    "    \n",
    "    def __init__(self, bounds):\n",
    "        super(Ackley_SGD, self).__init__()\n",
    "        self.dim = len(bounds)\n",
    "        sample = torch.Tensor(1, 1).uniform_(bounds[0][0], bounds[0][1])\n",
    "        for i in range(1, self.dim):\n",
    "            sample = torch.cat([sample, torch.Tensor(1, 1).uniform_(bounds[i][0], bounds[i][1])], dim=1)\n",
    "\n",
    "        self.x = nn.Parameter(sample)\n",
    "        \n",
    "    def forward(self):\n",
    "        a = 20; b = 0.2; c=2*math.pi;\n",
    "\n",
    "        term_1 = -a * torch.exp(-b*torch.sqrt(1/self.dim*torch.sum(self.x**2, dim=1)))\n",
    "        term_2 = -1*torch.exp(1/self.dim*torch.sum(torch.cos(c*self.x), dim=1))\n",
    "\n",
    "        return -1*(term_1 + term_2 + a + math.exp(1))\n",
    "    \n",
    "def Adam(f, bounds):\n",
    "    \n",
    "    tol = 10^-1\n",
    "    n_repeats = 10\n",
    "    dim = len(bounds)\n",
    "    opt_pt = Variable(torch.zeros(n_repeats, dim))\n",
    "    opt_val = Variable(torch.zeros(n_repeats))\n",
    "\n",
    "    for rep in range(n_repeats):\n",
    "\n",
    "        optimizer = torch.optim.Adam([{'params': f.x}], lr=0.01/dim)\n",
    "        prev_loss = 0\n",
    "        bound_flag = True\n",
    "\n",
    "        for t in range(10**4):\n",
    "            optimizer.zero_grad()\n",
    "            loss = -f()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "            if ((loss-prev_loss)**2 < tol).data.numpy(): break\n",
    "            prev_loss = loss\n",
    "\n",
    "        for i in range(dim):\n",
    "            if (f.x[:, i] < bounds[i][0]).any() or (f.x[:, i] > bounds[i][1]).any():\n",
    "                bound_flag = False\n",
    "                break\n",
    "\n",
    "        if bound_flag == True:\n",
    "            opt_val[rep] = f()\n",
    "            opt_pt[rep, :] = f.x\n",
    "            \n",
    "    return opt_pt, opt_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = [(-2, 2)]*3\n",
    "n_trials = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-3.3901\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "CPU times: user 1min 11s, sys: 5.51 s, total: 1min 17s\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "result_adam = Variable(torch.Tensor(n_trials))\n",
    "for t in range(n_trials):\n",
    "    f = Ackley_SGD(bounds)\n",
    "    opt_pt, opt_val = Adam(f, bounds)\n",
    "    result_adam[t] = opt_val[(opt_val <= 0).data].max()\n",
    "    \n",
    "print(result_adam.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GP initiated\n",
      "GP initiated\n",
      "GP initiated\n",
      "GP initiated\n",
      "GP initiated\n",
      "Variable containing:\n",
      "-1.3952\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "CPU times: user 1min 16s, sys: 464 ms, total: 1min 16s\n",
      "Wall time: 1min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "step_num = 100\n",
    "result_bayesopt = Variable(torch.Tensor(n_trials))\n",
    "for t in range(n_trials):\n",
    "    try:\n",
    "        test = Bayes_opt(func=test_functions.ackley, bounds=bounds, step_num=step_num)\n",
    "        test.optimize(mode=\"eval\")\n",
    "        result_bayesopt[t] = test.opt_val\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "#plt.plot(range(step_num), test.obj_hist.data.numpy())\n",
    "\n",
    "print(result_bayesopt.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam Results: Mean Optimal Value = -3.390089988708496, Variance = 4.63358736038208\n",
      "BayesOpt Results: Mean Optimal Value = -1.3951585292816162, Variance = 0.2216348499059677\n"
     ]
    }
   ],
   "source": [
    "print(\"Adam Results: Mean Optimal Value = {}, Variance = {}\".format(result_adam.mean().data[0], result_adam.var().data[0]))\n",
    "print(\"BayesOpt Results: Mean Optimal Value = {}, Variance = {}\".format(result_bayesopt.mean().data[0], result_bayesopt.var().data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-1.1398\n",
       "-2.1999\n",
       "-1.3294\n",
       "-0.9914\n",
       "-1.3152\n",
       "[torch.FloatTensor of size 5]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_bayesopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gpytorch]",
   "language": "python",
   "name": "conda-env-gpytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
